{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada91fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Uber BI template + color system loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROJECT ATLAS: MODULE 04a - SETUP AND DATA LOADING\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE: Shared environment and data loading for Risk Management modules\n",
    "# USAGE: %run ./04a_Setup_and_Data.ipynb in downstream notebooks\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ¬ß 1. ENVIRONMENT SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from typing import Optional, Dict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1.2 CONFIGURATION CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "AGG_DIR = './HVFHV subsets 2019-2025 - Aggregates/Aggregates_Processed/'\n",
    "SAMPLE_DIR = './HVFHV subsets 2019-2025 - Samples/'\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'executive': os.path.join(AGG_DIR, 'agg_executive_daily.csv'),\n",
    "    'sample_pattern': os.path.join(SAMPLE_DIR, 'tlc_sample_*_processed.parquet'),\n",
    "    'network': os.path.join(AGG_DIR, 'agg_network_monthly.parquet')\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# PLOTLY + UBER STYLE BOOTSTRAP\n",
    "# =============================================================================\n",
    "from pathlib import Path\n",
    "import plotly.io as pio\n",
    "\n",
    "import uber_style as ub \n",
    "\n",
    "pio.templates[\"uber\"] = ub.uber_style_template\n",
    "pio.templates.default = \"uber\"\n",
    "\n",
    "from uber_style import *\n",
    "\n",
    "# Export color constants explicitly for downstream notebooks\n",
    "UBER_BLACK = ub.UBER_BLACK\n",
    "UBER_RED = ub.UBER_RED\n",
    "UBER_GREEN = ub.UBER_GREEN\n",
    "UBER_ORANGE = ub.UBER_ORANGE\n",
    "UBER_PURPLE = ub.UBER_PURPLE\n",
    "UBER_BROWN = ub.UBER_BROWN\n",
    "UBER_YELLOW = ub.UBER_YELLOW\n",
    "UBER_WHITE = ub.UBER_WHITE\n",
    "GRAY_900 = ub.GRAY_900\n",
    "GRAY_700 = ub.GRAY_700\n",
    "GRAY_600 = ub.GRAY_600\n",
    "GRAY_500 = ub.GRAY_500\n",
    "GRAY_300 = ub.GRAY_300\n",
    "GRAY_100 = ub.GRAY_100\n",
    "\n",
    "# Create missing gray shades (interpolated)\n",
    "GRAY_800 = \"#222222\"  # Between GRAY_900 (#141414) and GRAY_700 (#333333)\n",
    "GRAY_400 = \"#CCCCCC\"  # Between GRAY_500 (#AFAFAF) and GRAY_300 (#E2E2E2)\n",
    "GRAY_200 = \"#ECECEC\"  # Between GRAY_300 (#E2E2E2) and GRAY_100 (#F6F6F6)\n",
    "\n",
    "PLOT_DIR = Path(\"plots\")\n",
    "PLOT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def _plot_paths(fig_name: str):\n",
    "    \"\"\"Return path json + html for 1 figure name.\"\"\"\n",
    "    json_path = PLOT_DIR / f\"{fig_name}.json\"\n",
    "    html_path = PLOT_DIR / f\"{fig_name}.html\"\n",
    "    return json_path, html_path\n",
    "\n",
    "\n",
    "def load_plot_if_exists(fig_name: str):\n",
    "    \"\"\"\n",
    "    If JSON file of the figure exists:\n",
    "        -> return (fig, True)\n",
    "    If not exists:\n",
    "        -> return (None, False)\n",
    "    \"\"\"\n",
    "    json_path, _ = _plot_paths(fig_name)\n",
    "    if json_path.exists():\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            fig = pio.from_json(f.read())\n",
    "        return fig, True\n",
    "    return None, False\n",
    "\n",
    "\n",
    "def save_plot(fig, fig_name: str):\n",
    "    \"\"\"\n",
    "    Save figure as JSON + HTML (no show).\n",
    "    \"\"\"\n",
    "    json_path, html_path = _plot_paths(fig_name)\n",
    "\n",
    "    # JSON\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pio.to_json(fig))\n",
    "\n",
    "    # HTML\n",
    "    pio.write_html(\n",
    "        fig,\n",
    "        file=str(html_path),\n",
    "        include_plotlyjs=\"cdn\",\n",
    "        auto_open=False\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Environment configured successfully\")\n",
    "print(f\"   - Notebook: 001a_Spatial_OD\")\n",
    "\n",
    "print(\"üé® Uber BI template + color system loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f25f1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Statistical utility functions loaded\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ¬ß 2. STATISTICAL UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def format_number(x: float, pos: Optional[int] = None) -> str:\n",
    "    \"\"\"Format large numbers with K/M suffixes.\"\"\"\n",
    "    if x >= 1e6:\n",
    "        return '{:1.1f}M'.format(x * 1e-6)\n",
    "    elif x >= 1e3:\n",
    "        return '{:1.0f}K'.format(x * 1e-3)\n",
    "    else:\n",
    "        return '{:1.0f}'.format(x)\n",
    "\n",
    "def format_currency(x: float, pos: Optional[int] = None) -> str:\n",
    "    \"\"\"Format currency values with $ prefix.\"\"\"\n",
    "    if x >= 1e6:\n",
    "        return '${:1.1f}M'.format(x * 1e-6)\n",
    "    elif x >= 1e3:\n",
    "        return '${:1.0f}K'.format(x * 1e-3)\n",
    "    else:\n",
    "        return '${:1.0f}'.format(x)\n",
    "\n",
    "def calculate_modified_zscore(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate modified Z-score using median absolute deviation (MAD).\n",
    "    More robust to outliers than standard Z-score.\n",
    "    \n",
    "    Args:\n",
    "        data: 1D numpy array\n",
    "    \n",
    "    Returns:\n",
    "        Modified Z-scores for each data point\n",
    "    \"\"\"\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    modified_z = 0.6745 * (data - median) / mad if mad > 0 else np.zeros_like(data)\n",
    "    return modified_z\n",
    "\n",
    "def flag_outliers_iqr(df: pl.DataFrame, column: str, \n",
    "                      multiplier: float = 3.0) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag outliers using IQR method.\n",
    "    \n",
    "    Args:\n",
    "        df: Polars DataFrame\n",
    "        column: Column name to analyze\n",
    "        multiplier: IQR multiplier (1.5 = moderate, 3.0 = extreme)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outlier flag column\n",
    "    \"\"\"\n",
    "    q1 = df.select(pl.col(column).quantile(0.25)).item()\n",
    "    q3 = df.select(pl.col(column).quantile(0.75)).item()\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "    \n",
    "    df_flagged = df.with_columns([\n",
    "        ((pl.col(column) < lower_bound) | (pl.col(column) > upper_bound))\n",
    "        .alias(f'{column}_outlier')\n",
    "    ])\n",
    "    \n",
    "    return df_flagged\n",
    "\n",
    "print(\"‚úÖ Statistical utility functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769c2fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ¬ß 3. DATA LOADING FUNCTIONS (MEMORY-OPTIMIZED)\n",
    "# =============================================================================\n",
    "\n",
    "def load_full_sample_data(pattern: str, use_lazy: bool = True, n_rows: Optional[int] = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and merge all sample parquet files with full feature set.\n",
    "    Uses lazy evaluation for memory efficiency with large datasets (5M+ rows).\n",
    "    \n",
    "    Args:\n",
    "        pattern: Glob pattern for sample files\n",
    "        use_lazy: If True, use lazy evaluation (scan_parquet) for memory efficiency\n",
    "        n_rows: Optional row limit for testing (None = load all)\n",
    "    \n",
    "    Returns:\n",
    "        Combined Polars DataFrame with all trips\n",
    "    \"\"\"\n",
    "    sample_files = sorted(glob.glob(pattern))\n",
    "    \n",
    "    if not sample_files:\n",
    "        raise FileNotFoundError(f\"No files found matching pattern: {pattern}\")\n",
    "    \n",
    "    print(f\"   üìÇ Located {len(sample_files)} sample files\")\n",
    "    for f in sample_files:\n",
    "        print(f\"      - {os.path.basename(f)}\")\n",
    "    \n",
    "    # Use lazy evaluation for memory efficiency\n",
    "    if use_lazy and n_rows is None:\n",
    "        print(\"   üîß Using lazy evaluation (scan_parquet) for memory efficiency...\")\n",
    "        df_lazy = pl.scan_parquet(sample_files)\n",
    "        \n",
    "        # Apply filters in lazy mode before collecting\n",
    "        df_lazy = df_lazy.filter(\n",
    "            (pl.col('trip_km') > 0) &\n",
    "            (pl.col('duration_min') > 0) &\n",
    "            (pl.col('total_rider_cost') > 0) &\n",
    "            (pl.col('speed_kmh') > 0) &\n",
    "            (pl.col('speed_kmh') <= 120)\n",
    "        )\n",
    "        \n",
    "        df = df_lazy.collect()\n",
    "    else:\n",
    "        # For testing with row limit or explicit eager loading\n",
    "        if n_rows:\n",
    "            print(f\"   üß™ Test mode: Loading first {n_rows:,} rows only...\")\n",
    "        df = pl.read_parquet(sample_files, n_rows=n_rows)\n",
    "        \n",
    "        # Apply data quality filters\n",
    "        df = df.filter(\n",
    "            (pl.col('trip_km') > 0) &\n",
    "            (pl.col('duration_min') > 0) &\n",
    "            (pl.col('total_rider_cost') > 0) &\n",
    "            (pl.col('speed_kmh') > 0) &\n",
    "            (pl.col('speed_kmh') <= 120)\n",
    "        )\n",
    "    \n",
    "    # Validate critical columns for anomaly analysis\n",
    "    required_cols = [\n",
    "        'trip_km', 'duration_min', 'total_rider_cost',\n",
    "        'pickup_datetime', 'pickup_borough', 'dropoff_borough',\n",
    "        'speed_kmh'\n",
    "    ]\n",
    "    \n",
    "    # Optional columns (weather data)\n",
    "    optional_cols = ['conditions', 'temp', 'weather_state']\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing critical columns: {missing}\")\n",
    "    \n",
    "    # Check weather columns availability\n",
    "    weather_available = all(col in df.columns for col in optional_cols)\n",
    "    if not weather_available:\n",
    "        print(f\"   ‚ö†Ô∏è  Weather columns not fully available: {[c for c in optional_cols if c not in df.columns]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_executive_daily_from_sample(df_sample: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate daily executive metrics from trip-level sample data.\n",
    "    This is the CORRECT way to get actual speed metrics instead of using placeholders.\n",
    "    \n",
    "    Args:\n",
    "        df_sample: Full trip-level sample data\n",
    "    \n",
    "    Returns:\n",
    "        Daily aggregated metrics with actual calculated values\n",
    "    \"\"\"\n",
    "    print(\"   üîß Calculating daily metrics from trip-level data...\")\n",
    "    \n",
    "    # Extract date from datetime\n",
    "    daily_agg = (\n",
    "        df_sample\n",
    "        .with_columns([\n",
    "            pl.col('pickup_datetime').dt.date().alias('date')\n",
    "        ])\n",
    "        .group_by('date')\n",
    "        .agg([\n",
    "            pl.count().alias('total_trips'),\n",
    "            pl.col('total_rider_cost').sum().alias('total_revenue'),\n",
    "            pl.col('total_rider_cost').sum().alias('total_gross_booking_value'),\n",
    "            pl.col('trip_km').sum().alias('total_km_traveled'),\n",
    "            \n",
    "            # ACTUAL calculated metrics (not placeholders)\n",
    "            pl.col('speed_kmh').mean().alias('avg_speed_kmh'),\n",
    "            pl.col('speed_kmh').std().alias('std_speed_kmh'),\n",
    "            pl.col('duration_min').mean().alias('avg_duration_min'),\n",
    "            pl.col('total_rider_cost').mean().alias('avg_cost'),\n",
    "            pl.col('trip_km').mean().alias('avg_distance_km'),\n",
    "            \n",
    "            # Weather impact metrics (if available)\n",
    "            *([pl.col('conditions').mode().first().alias('dominant_weather')] \n",
    "              if 'conditions' in df_sample.columns else []),\n",
    "            *([pl.col('temp').mean().alias('avg_temp')] \n",
    "              if 'temp' in df_sample.columns else [])\n",
    "        ])\n",
    "        .sort('date')\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Calculated {daily_agg.height:,} days of metrics\")\n",
    "    return daily_agg\n",
    "\n",
    "print(\"‚úÖ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e089a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚è≥ LOADING DATA FOR RISK MANAGEMENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Loading Full Sample Data (tlc_sample_*_processed)...\n",
      "   üí° Using lazy evaluation to handle 5M+ rows efficiently...\n",
      "   üìÇ Located 7 sample files\n",
      "      - tlc_sample_2019_processed.parquet\n",
      "      - tlc_sample_2020_processed.parquet\n",
      "      - tlc_sample_2021_processed.parquet\n",
      "      - tlc_sample_2022_processed.parquet\n",
      "      - tlc_sample_2023_processed.parquet\n",
      "      - tlc_sample_2024_processed.parquet\n",
      "      - tlc_sample_2025_processed.parquet\n",
      "   üîß Using lazy evaluation (scan_parquet) for memory efficiency...\n",
      "\n",
      "   ‚úÖ Loaded: 9,830,241 trips\n",
      "\n",
      "   ‚úÖ Loaded: 9,830,241 trips\n",
      "   üíæ Memory footprint: 3694.2 MB\n",
      "   üìÖ Date range: 2019-02-01 00:00:16 to 2025-09-30 23:58:55\n",
      "\n",
      "üìä Calculating Daily Executive Metrics from Sample...\n",
      "   üîß Calculating daily metrics from trip-level data...\n",
      "   üíæ Memory footprint: 3694.2 MB\n",
      "   üìÖ Date range: 2019-02-01 00:00:16 to 2025-09-30 23:58:55\n",
      "\n",
      "üìä Calculating Daily Executive Metrics from Sample...\n",
      "   üîß Calculating daily metrics from trip-level data...\n",
      "   ‚úÖ Calculated 2,433 days of metrics\n",
      "   ‚úÖ Generated 2,433 daily records with ACTUAL speed metrics\n",
      "   üìä Avg Speed Range: 17.9 - 30.3 km/h\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DATA LOADING COMPLETE - Ready for analysis modules\n",
      "================================================================================\n",
      "\n",
      "üí° Available datasets:\n",
      "   - df_sample : Trip-level data (5M+ rows)\n",
      "   - df_daily  : Daily aggregates (2K+ days)\n",
      "\n",
      "‚ö° Use %run ./04a_Setup_and_Data.ipynb in other notebooks to import\n",
      "   ‚úÖ Calculated 2,433 days of metrics\n",
      "   ‚úÖ Generated 2,433 daily records with ACTUAL speed metrics\n",
      "   üìä Avg Speed Range: 17.9 - 30.3 km/h\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DATA LOADING COMPLETE - Ready for analysis modules\n",
      "================================================================================\n",
      "\n",
      "üí° Available datasets:\n",
      "   - df_sample : Trip-level data (5M+ rows)\n",
      "   - df_daily  : Daily aggregates (2K+ days)\n",
      "\n",
      "‚ö° Use %run ./04a_Setup_and_Data.ipynb in other notebooks to import\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ¬ß 4. LOAD DATA (EXECUTE ONCE)\n",
    "# =============================================================================\n",
    "\n",
    "# Check if data already loaded (prevent re-loading on %run)\n",
    "if 'df_sample' not in globals() or 'df_daily' not in globals():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚è≥ LOADING DATA FOR RISK MANAGEMENT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        # Load full sample data with lazy evaluation\n",
    "        print(\"\\nüìä Loading Full Sample Data (tlc_sample_*_processed)...\")\n",
    "        print(\"   üí° Using lazy evaluation to handle 5M+ rows efficiently...\")\n",
    "        \n",
    "        df_sample = load_full_sample_data(\n",
    "            DATA_PATHS['sample_pattern'],\n",
    "            use_lazy=True,  # Memory-efficient lazy loading\n",
    "            n_rows=None     # Change to e.g. 100000 for testing\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ Loaded: {df_sample.height:,} trips\")\n",
    "        print(f\"   üíæ Memory footprint: {df_sample.estimated_size('mb'):.1f} MB\")\n",
    "        print(f\"   üìÖ Date range: {df_sample.select(pl.col('pickup_datetime').min())[0,0]} to {df_sample.select(pl.col('pickup_datetime').max())[0,0]}\")\n",
    "        \n",
    "        # Calculate daily metrics FROM SAMPLE DATA (correct approach)\n",
    "        print(\"\\nüìä Calculating Daily Executive Metrics from Sample...\")\n",
    "        df_daily = calculate_executive_daily_from_sample(df_sample)\n",
    "        \n",
    "        print(f\"   ‚úÖ Generated {df_daily.height:,} daily records with ACTUAL speed metrics\")\n",
    "        print(f\"   üìä Avg Speed Range: {df_daily['avg_speed_kmh'].min():.1f} - {df_daily['avg_speed_kmh'].max():.1f} km/h\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ DATA LOADING COMPLETE - Ready for analysis modules\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nüí° Available datasets:\")\n",
    "        print(\"   - df_sample : Trip-level data (5M+ rows)\")\n",
    "        print(\"   - df_daily  : Daily aggregates (2K+ days)\")\n",
    "        print(\"\\n‚ö° Use %run ./04a_Setup_and_Data.ipynb in other notebooks to import\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: Data loading failed\")\n",
    "        print(f\"   Details: {str(e)}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n‚úÖ Data already loaded in memory - skipping reload\")\n",
    "    print(f\"   - df_sample: {df_sample.height:,} trips ({df_sample.estimated_size('mb'):.1f} MB)\")\n",
    "    print(f\"   - df_daily: {df_daily.height:,} daily records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
